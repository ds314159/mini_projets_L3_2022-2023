AIC     = c(aic_1, aic_2),
BIC     = c(bic_1, bic_2)
)
tableau
plot(fitted(lmod_2), residuals(lmod_2), xlab="Fitted", ylab="Residuals")
abline(h=0, col="red")
summary(lm(sqrt(abs(residuals(lmod_2)))~fitted(lmod_2)))
qqnorm(residuals(lmod_2), ylab="Residuals" , main="QQnorm des erreurs")
qqline(residuals(lmod_2), col="green")
shapiro.test(residuals(lmod_2))
# obtention de l'influence de chaque observation sur l'ajustement
mesures_leviers = hatvalues(lmod_2)
# plotter le quantile normal théorique de la mesure
halfnorm(mesures_leviers, ylab= "leviers")
# calculer le Seuil
seuil = (2*9)/1674
# récuperer les points levier identifiés numériquement
mesures_leviers_retenus = data.frame(mesures_leviers[mesures_leviers > seuil])
dim(mesures_leviers_retenus)
alpha = 0.05
df = 1665
n = 1674
stud = rstudent(lmod_2)
seuil_critique_bonferoni_abs = abs(qt(alpha/(n*2), df))
outliers = which(abs(stud) > seuil_critique_bonferoni_abs)
length(outliers)
cook = cooks.distance(lmod_2)
halfnorm(cook)
par(mfrow=c(2,2))
plot(lmod_2)
par(mfcol = c(3, 3))
# Par variable ( terms = num var indep)
termplot(lmod_2, partial.resid = TRUE, terms = 1)
termplot(lmod_2, partial.resid = TRUE, terms = 2)
termplot(lmod_2, partial.resid = TRUE, terms = 3)
termplot(lmod_2, partial.resid = TRUE, terms = 4)
termplot(lmod_2, partial.resid = TRUE, terms = 5)
termplot(lmod_2, partial.resid = TRUE, terms = 6)
termplot(lmod_2, partial.resid = TRUE, terms = 7)
termplot(lmod_2, partial.resid = TRUE, terms = 8)
termplot(lmod_2, partial.resid = TRUE, terms = 9)
alpha = 0.05
model = lmod_2
# Fixer le nombre de tirages:
nbr_tirages = 4000
# Créer la matrice qui contiendra les coeffs Beta de chaque itération
p = length(model$coefficients)
matrice_coeffs = matrix(NA, nbr_tirages, p)
colnames(matrice_coeffs) = names(model$coefficients)
# récupérer les residuals et les fitted
residuals = residuals(model)
fitted = fitted(model)
# Réaliser les tirages avec remise et remplir la matrice des coeffs
for(i in 1:nbr_tirages){
booty = fitted + sample(residuals,  rep=TRUE)
model_i = update(model, booty ~ .)
matrice_coeffs[i,]= coef(model_i)
}
# Obtenir l'intervalle de confiance pour chaque predicteur
CIs = apply(matrice_coeffs, 2, function(x) quantile(x, c((alpha/2), 1-(alpha/2))))
CIS = t(CIs)
CIS = cbind(CIS, coefficients(lmod_2))
colnames(CIS)[1] = "seuil 2.5% "
colnames(CIS)[2] = "seuil 97.5%"
colnames(CIS)[3] = "valeur modele"
kable(CIS)
Lmod_non_transformee = lm(ozone~., data =training.set[, 2:18])
boxcox(Lmod_non_transformee, lambda = seq(-2, 2, by = 0.1))
boxcox(Lmod_non_transformee, lambda = seq(0.4, 0.7, by = 0.1))
lmod_transformee= lm(ozone^0.525~. , data =training.set[, 2:18])
par(mfrow = c(2,2))
plot(lmod_transformee)
shapiro.test(lmod_transformee$residuals)
lmod_transformee_plus1 = backwards(lmod_transformee, 0.05)
# récupérer les variables significatives du jeu training
training.set.final = training.set[, c(2,3,4,6,7,8,10,11,12,13, 14,15,17,18)]
# supprimer les points leviers :
leviers = hatvalues(lmod_transformee_plus1)
seuil = (2*14)/1674
leviers_retenus = data.frame(leviers[leviers > seuil])
training.set.final = training.set.final[!(rownames(training.set.final) %in% rownames(leviers_retenus)),]
# supprimer mesures aberrantes :
alpha = 0.05
df = 1550
n = 1564 # les set est déjà débarrassé des 110 points leviers
stud = rstudent(lmod_transformee_plus1)
seuil_critique_bonferoni_abs = abs(qt(alpha/(n*2), df))
outliers = which(abs(stud) > seuil_critique_bonferoni_abs)
length(outliers) #  = 0 , => zéro outliers trouvés
# supprimer les points d'influence
cook = cooks.distance(lmod_transformee_plus1)
indices_influents = which(cook > 4/1564) # règle empirique
training.set.final = training.set.final[-indices_influents, ]
final_model = backwards(lm(ozone^0.525~., data = training.set.final), 0.05)
par(mfrow=c(2,2))
plot(final_model)
summary(final_model)
ozone_observation = testing.set[,2]
ozone_prediction = data.frame(predict.lm(final_model, testing.set))
par(mfrow=c(1,2))
stat1=summary(ozone_observation)
stat2=summary(ozone_prediction)
stat = cbind(stat1, stat2)
colnames(stat)= c("ozone observé", "ozone prédit")
kable(stat)
plot(ozone_observation, type="l", col="blue",
xlab="du 01/01/2023 au 01/04/2023",
ylab="Ozone observé vs ozone prédit")
lines(ozone_prediction, col="red")
rmse = RMSE(ozone_prediction[,1], ozone_observation)
rmse
mape = MAPE( ozone_prediction[,1], ozone_observation)
mape
set.seed(909)
# Librairies:
library("forecast")
library("MLmetrics")
library("genridge")
library("leaps")
library("MASS")
library("faraway")
library("splines")
library("markdown")
library("knitr")
library("rmarkdown")
library("GGally")
library("FactoMineR")
library("factoextra")
library("missMDA")
library("corrplot")
library("FactoInvestigate")
library("readxl")
library("graphics")
library("gplots")
library("questionr")
library("ExPosition")
library("raster")
library("rgdal")
library("dbplyr")
library("stats")
library("readr")
library("utils")
library("ggplot2")
library("faraway")
library("car")
library("MASS")
library("quarto")
d.meteo = read_excel("meteo_albertville_2023-01-01_2023-04-01.xls",
col_types = c("text", "date", "numeric",
"numeric", "numeric", "numeric",
"numeric", "numeric", "numeric",
"numeric", "numeric"))
d.meteo = d.meteo[grepl("00:00$", d.meteo$dh_utc), c(2,3,4,5,6,7,9,11)]
d.ozone = read_csv("ozone_Albertville_2023-01-01_2023-04-01.csv",
col_types = c("T", "numeric", "text"))
d.PM2.5 = read.csv("particules PM2,5_Albertville_2023-01-01_2023-04-01.csv")
d.PM10  = read.csv("particules PM10_Albertville_2023-01-01_2023-04-01.csv")
d.NO2   = read.csv("dioxyde d'azote_Albertville_2023-01-01_2023-04-01.csv")
d.NO    = read.csv("monoxyde d'azote_Albertville_2023-01-01_2023-04-01.csv")
d.ozone.lag = data.frame(dplyr::lag(d.ozone$ozone..µg.m.., 24))
d.PM2.5.lag = data.frame(dplyr::lag(d.PM2.5$particules.PM2.5..µg.m.., 24))
d.PM10.lag  = data.frame(dplyr::lag(d.PM10$particules.PM10..µg.m.., 24))
d.NO2.lag   = data.frame(dplyr::lag(d.NO2$dioxyde.d.azote..µg.m.., 24))
d.NO.lag    = data.frame(dplyr::lag(d.NO$monoxyde.d.azote..µg.m.., 24))
d.polluants    =    cbind(d.ozone[,1:2],
d.ozone.lag ,
d.NO[,2],
d.NO.lag ,
d.NO2[,2],
d.NO2.lag,
d.PM2.5[,2],
d.PM2.5.lag ,
d.PM10[,2],
d.PM10.lag)
colnames(d.polluants) = c("datetime",
"ozone",
"ozone_lag",
"no",
"no_lag",
"no2",
"no2_lag",
"pm2.5",
"pm2.5_lag",
"pm10",
"pm10_lag")
data = merge(d.polluants, d.meteo, by.x = "datetime", by.y = "dh_utc")
data = data %>% dplyr::mutate(weekday = weekdays(datetime))
data$weekday = as.factor(data$weekday)
data = na.omit(data)
nrows.training = floor(0.8 * nrow(data))
training.indices = sample(seq_len(nrow(data)), size = nrows.training)
training.set = data[training.indices, ]
testing.set  = data[-training.indices, ]
save(training.set, file = "training_set.RData")
save(testing.set, file = "testing_set.RData")
kable(training.set[1:6,1:10])
kable(training.set[1:6,11:15])
kable(training.set[1:6,16:19])
summary(training.set)
ggpairs(training.set[, c(2,4,6,8,10)])
ggpairs(training.set[, c(2,3,5,7,9,11)])
ggpairs(training.set[, c(2,12,13,14,16,17,18)])
ggpairs(training.set[, c(2,19)], aes(color=weekday))
# on stocke dans dtcr ( data training centrée réduite) les données d'entrainement centrées réduites
dtcr = data.table::copy(training.set)
moyennes = apply(dtcr[ ,2:18], 2, mean)
ecarts_types_biaisés = apply(dtcr[ ,2:18], 2, sd) * (sqrt(1673/1674))
centree = sweep(dtcr[ , 2:18], 2, moyennes, "-")
centree_reduite = sweep(centree, 2, ecarts_types_biaisés, "/")
dtcr[ , 2:18] = centree_reduite
lmod_0 = lm(ozone~., data = dtcr[, 2:18]) # colonne datetime non incluse
summary(lmod_0)
# inspirée du TD 6 , et du cours de la programmation objet
backwards = function(model, alpha){
resultat = model
condition = TRUE
while (condition) {
sommaire = summary(resultat)
pvals = sommaire$coefficients[,4]
nom_colonne = names(sort(pvals[pvals > alpha], decreasing = TRUE)[1])
condition = (is.na(nom_colonne)==FALSE)
if (condition){
if (nom_colonne == "(Intercept)"){nom_colonne = 1 }
formule = paste0(". ~ . - ",nom_colonne)
resultat = update(resultat, formule)
}
}
return(resultat)
}
alpha = 0.05
lmod_1 = backwards(lmod_0, alpha)
summary(lmod_1)
alpha = 0.0001
lmod_2 = backwards(lmod_0, alpha)
summary(lmod_2)
anova(lmod_2, lmod_1)
# proposition de fonction pour calculer le critère de mallow:
Cp = function(modelComplet, modelPartiel ){
summary1 = summary(modelComplet)
summary2 = summary(modelPartiel)
RSS_p = sum(summary2$residuals^2)
sigma2= summary1$sigma^2
n = nrow(modelPartiel$model)
p = ncol(modelPartiel$model)
mallow = (RSS_p/sigma2)-(n-(2*p))
return(mallow)
}
# critère de Mallow:
mallow_1 = Cp(lmod_0, lmod_1)
mallow_2 = Cp(lmod_0, lmod_2)
# critère AKAIKE:
aic_1 = AIC(lmod_1)
aic_2 = AIC(lmod_2)
# critère BIC :
bic_1 = BIC(lmod_1)
bic_2 = BIC(lmod_2)
# tableau :
tableau = data.frame( modeles = c("lmod_1", "lmod_2"),
MALLOW  = c(mallow_1, mallow_2),
AIC     = c(aic_1, aic_2),
BIC     = c(bic_1, bic_2)
)
tableau
plot(fitted(lmod_2), residuals(lmod_2), xlab="Fitted", ylab="Residuals")
abline(h=0, col="red")
summary(lm(sqrt(abs(residuals(lmod_2)))~fitted(lmod_2)))
qqnorm(residuals(lmod_2), ylab="Residuals" , main="QQnorm des erreurs")
qqline(residuals(lmod_2), col="green")
shapiro.test(residuals(lmod_2))
# obtention de l'influence de chaque observation sur l'ajustement
mesures_leviers = hatvalues(lmod_2)
# plotter le quantile normal théorique de la mesure
halfnorm(mesures_leviers, ylab= "leviers")
# calculer le Seuil
seuil = (2*9)/1674
# récuperer les points levier identifiés numériquement
mesures_leviers_retenus = data.frame(mesures_leviers[mesures_leviers > seuil])
dim(mesures_leviers_retenus)
alpha = 0.05
df = 1665
n = 1674
stud = rstudent(lmod_2)
seuil_critique_bonferoni_abs = abs(qt(alpha/(n*2), df))
outliers = which(abs(stud) > seuil_critique_bonferoni_abs)
length(outliers)
cook = cooks.distance(lmod_2)
halfnorm(cook)
par(mfrow=c(2,2))
plot(lmod_2)
par(mfcol = c(3, 3))
# Par variable ( terms = num var indep)
termplot(lmod_2, partial.resid = TRUE, terms = 1)
termplot(lmod_2, partial.resid = TRUE, terms = 2)
termplot(lmod_2, partial.resid = TRUE, terms = 3)
termplot(lmod_2, partial.resid = TRUE, terms = 4)
termplot(lmod_2, partial.resid = TRUE, terms = 5)
termplot(lmod_2, partial.resid = TRUE, terms = 6)
termplot(lmod_2, partial.resid = TRUE, terms = 7)
termplot(lmod_2, partial.resid = TRUE, terms = 8)
termplot(lmod_2, partial.resid = TRUE, terms = 9)
alpha = 0.05
model = lmod_2
# Fixer le nombre de tirages:
nbr_tirages = 4000
# Créer la matrice qui contiendra les coeffs Beta de chaque itération
p = length(model$coefficients)
matrice_coeffs = matrix(NA, nbr_tirages, p)
colnames(matrice_coeffs) = names(model$coefficients)
# récupérer les residuals et les fitted
residuals = residuals(model)
fitted = fitted(model)
# Réaliser les tirages avec remise et remplir la matrice des coeffs
for(i in 1:nbr_tirages){
booty = fitted + sample(residuals,  rep=TRUE)
model_i = update(model, booty ~ .)
matrice_coeffs[i,]= coef(model_i)
}
# Obtenir l'intervalle de confiance pour chaque predicteur
CIs = apply(matrice_coeffs, 2, function(x) quantile(x, c((alpha/2), 1-(alpha/2))))
CIS = t(CIs)
CIS = cbind(CIS, coefficients(lmod_2))
colnames(CIS)[1] = "seuil 2.5% "
colnames(CIS)[2] = "seuil 97.5%"
colnames(CIS)[3] = "valeur modele"
kable(CIS)
Lmod_non_transformee = lm(ozone~., data =training.set[, 2:18])
boxcox(Lmod_non_transformee, lambda = seq(-2, 2, by = 0.1))
boxcox(Lmod_non_transformee, lambda = seq(0.4, 0.7, by = 0.1))
lmod_transformee= lm(ozone^0.525~. , data =training.set[, 2:18])
par(mfrow = c(2,2))
plot(lmod_transformee)
shapiro.test(lmod_transformee$residuals)
lmod_transformee_plus1 = backwards(lmod_transformee, 0.05)
# récupérer les variables significatives du jeu training
training.set.final = training.set[, c(2,3,4,6,7,8,10,11,12,13, 14,15,17,18)]
# supprimer les points leviers :
leviers = hatvalues(lmod_transformee_plus1)
seuil = (2*14)/1674
leviers_retenus = data.frame(leviers[leviers > seuil])
training.set.final = training.set.final[!(rownames(training.set.final) %in% rownames(leviers_retenus)),]
# supprimer mesures aberrantes :
alpha = 0.05
df = 1550
n = 1564 # les set est déjà débarrassé des 110 points leviers
stud = rstudent(lmod_transformee_plus1)
seuil_critique_bonferoni_abs = abs(qt(alpha/(n*2), df))
outliers = which(abs(stud) > seuil_critique_bonferoni_abs)
length(outliers) #  = 0 , => zéro outliers trouvés
# supprimer les points d'influence
cook = cooks.distance(lmod_transformee_plus1)
indices_influents = which(cook > 4/1564) # règle empirique
training.set.final = training.set.final[-indices_influents, ]
final_model = backwards(lm(ozone^0.525~., data = training.set.final), 0.05)
par(mfrow=c(2,2))
plot(final_model)
summary(final_model)
ozone_observation = testing.set[,2]
ozone_prediction = data.frame(predict.lm(final_model, testing.set))
par(mfrow=c(1,2))
stat1=summary(ozone_observation)
stat2=summary(ozone_prediction)
stat = cbind(stat1, stat2)
colnames(stat)= c("ozone observé", "ozone prédit")
kable(stat)
plot(ozone_observation, type="l", col="blue",
xlab="du 01/01/2023 au 01/04/2023",
ylab="Ozone observé vs ozone prédit")
lines(ozone_prediction, col="red")
rmse = RMSE(ozone_prediction[,1], ozone_observation)
rmse
mape = MAPE( ozone_prediction[,1], ozone_observation)
mape
alpha = 0.05
model = lmod_2
# Fixer le nombre de tirages :
nbr_tirages = 4000
# Garder le même résultat si réexecution de la chunk
set.seed(123)
# Créer la matrice qui contiendra les coeffs Beta de chaque itération
p = length(model$coefficients)
matrice_coeffs = matrix(NA, nbr_tirages, p)
colnames(matrice_coeffs) = names(model$coefficients)
# récupérer les residuals et les fitted
residuals = residuals(model)
fitted = fitted(model)
# Réaliser les tirages avec remise et remplir la matrice des coeffs
for(i in 1:nbr_tirages){
booty = fitted + sample(residuals,  rep=TRUE)
model_i = update(model, booty ~ .)
matrice_coeffs[i,]= coef(model_i)
}
# Obtenir l'intervalle de confiance pour chaque predicteur
CIs = apply(matrice_coeffs, 2, function(x) quantile(x, c((alpha/2), 1-(alpha/2))))
CIS = t(CIs)
CIS = cbind(CIS, coefficients(lmod_2))
colnames(CIS)[1] = "seuil 2.5% "
colnames(CIS)[2] = "seuil 97.5%"
colnames(CIS)[3] = "valeur modele"
kable(CIS)
Lmod_non_transformee = lm(ozone~., data =training.set[, 2:18])
boxcox(Lmod_non_transformee, lambda = seq(-2, 2, by = 0.1))
boxcox(Lmod_non_transformee, lambda = seq(0.4, 0.7, by = 0.1))
boxcox(Lmod_non_transformee, lambda = seq(0.5, 0.55, by = 0.1))
boxcox(Lmod_non_transformee, lambda = seq(0.5, 0.55, by = 0.01))
lmod_transformee= lm(ozone^0.537~. , data =training.set[, 2:18])
par(mfrow = c(2,2))
plot(lmod_transformee)
boxcox(Lmod_non_transformee, lambda = seq(0.4, 0.7, by = 0.1))
lmod_transformee= lm(ozone^0.537~. , data =training.set[, 2:18])
par(mfrow = c(2,2))
plot(lmod_transformee)
shapiro.test(lmod_transformee$residuals)
lmod_transformee_plus1 = backwards(lmod_transformee, 0.05)
# récupérer les variables significatives du jeu training
training.set.final = training.set[, c(2,3,4,6,7,8,10,11,12,13, 14,15,17,18)]
# supprimer les points leviers :
leviers = hatvalues(lmod_transformee_plus1)
seuil = (2*14)/1674
leviers_retenus = data.frame(leviers[leviers > seuil])
training.set.final = training.set.final[!(rownames(training.set.final) %in% rownames(leviers_retenus)),]
# supprimer mesures aberrantes :
alpha = 0.05
df = 1550
n = 1564 # les set est déjà débarrassé des 110 points leviers
stud = rstudent(lmod_transformee_plus1)
seuil_critique_bonferoni_abs = abs(qt(alpha/(n*2), df))
outliers = which(abs(stud) > seuil_critique_bonferoni_abs)
length(outliers) #  = 0 , => zéro outliers trouvés
# supprimer les points d'influence
cook = cooks.distance(lmod_transformee_plus1)
indices_influents = which(cook > 4/1564) # règle empirique
training.set.final = training.set.final[-indices_influents, ]
plot(final_model)
final_model = backwards(lm(ozone^0.525~., data = training.set.final), 0.05)
par(mfrow=c(2,2))
plot(final_model)
final_model = backwards(lm(ozone^0.537~., data = training.set.final), 0.05)
par(mfrow=c(2,2))
plot(final_model)
plot(final_model)
final_model = backwards(lm(ozone^0.537~., data = training.set.final), 0.05)
par(mfrow=c(2,2))
plot(final_model)
summary(final_model)
ozone_observation = testing.set[,2]
ozone_prediction = data.frame(predict.lm(final_model, testing.set))
par(mfrow=c(1,2))
stat1=summary(ozone_observation)
stat2=summary(ozone_prediction)
stat = cbind(stat1, stat2)
colnames(stat)= c("ozone observé", "ozone prédit")
kable(stat)
rmse = RMSE(ozone_prediction[,1], ozone_observation)
rmse
mape = MAPE( ozone_prediction[,1], ozone_observation)
mape
render("prediction_ozone.qmd", clean=FALSE)
?render
# proposition de fonction pour calculer le critère de mallow:
Cp = function(modelComplet, modelPartiel ){
summary1 = summary(modelComplet)
summary2 = summary(modelPartiel)
RSS_p = sum(summary2$residuals^2)
sigma2= summary1$sigma^2
n = nrow(modelPartiel$model)
p = ncol(modelPartiel$model)
mallow = (RSS_p/sigma2)-(n-(2*p))
return(mallow)
}
# critère de Mallow:
mallow_1 = Cp(lmod_0, lmod_1)
mallow_2 = Cp(lmod_0, lmod_2)
# critère AKAIKE:
aic_1 = AIC(lmod_1)
aic_2 = AIC(lmod_2)
# critère BIC :
bic_1 = BIC(lmod_1)
bic_2 = BIC(lmod_2)
# tableau :
tableau = data.frame( modeles = c("lmod_1", "lmod_2"),
MALLOW  = c(mallow_1, mallow_2),
AIC     = c(aic_1, aic_2),
BIC     = c(bic_1, bic_2)
)
tableau
# proposition de fonction pour calculer le critère de mallow:
Cp = function(modelComplet, modelPartiel ){
summary1 = summary(modelComplet)
summary2 = summary(modelPartiel)
RSS_p = sum(summary2$residuals^2)
sigma2= summary1$sigma^2
n = nrow(modelPartiel$model)
p = ncol(modelPartiel$model)
mallow = (RSS_p/sigma2)-(n-(2*p))
return(mallow)
}
# critère de Mallow:
mallow_1 = Cp(lmod_0, lmod_1)
mallow_2 = Cp(lmod_0, lmod_2)
# critère AKAIKE:
aic_1 = AIC(lmod_1)
aic_2 = AIC(lmod_2)
# critère BIC :
bic_1 = BIC(lmod_1)
bic_2 = BIC(lmod_2)
# tableau :
tableau = data.frame( modeles = c("lmod_1", "lmod_2"),
MALLOW  = c(mallow_1, mallow_2),
AIC     = c(aic_1, aic_2),
BIC     = c(bic_1, bic_2)
)
tableau
